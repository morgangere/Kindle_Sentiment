{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b7c0d",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c8b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\preprocessed_kindle_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c129ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "      <td>50 + years ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "      <td>Boring! Boring! Boring!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "      <td>Wiggleliscious/new toy ready/!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I don't normally buy 'mystery' novels because ...</td>\n",
       "      <td>Very good read.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This isn't the kind of book I normally read, a...</td>\n",
       "      <td>Great Story!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>2</td>\n",
       "      <td>Had to read certain passages twice--typos.  Wi...</td>\n",
       "      <td>Where's the meat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>3</td>\n",
       "      <td>Not what i expected. yet a very interesting bo...</td>\n",
       "      <td>Interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>5</td>\n",
       "      <td>Dragon Knights is a world where Knights ride d...</td>\n",
       "      <td>Dragon Knights, Wings of Change (I Dream of Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>4</td>\n",
       "      <td>Since this story is very short, it's hard to s...</td>\n",
       "      <td>Good writing, short story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>11999</td>\n",
       "      <td>4</td>\n",
       "      <td>from 1922 an amazing collection of info on sym...</td>\n",
       "      <td>interesting public domain book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  rating                                         reviewText  \\\n",
       "0               0       5  This book was the very first bookmobile book I...   \n",
       "1               1       1  When I read the description for this book, I c...   \n",
       "2               2       5  I just had to edit this review. This book is a...   \n",
       "3               3       5  I don't normally buy 'mystery' novels because ...   \n",
       "4               4       5  This isn't the kind of book I normally read, a...   \n",
       "...           ...     ...                                                ...   \n",
       "11995       11995       2  Had to read certain passages twice--typos.  Wi...   \n",
       "11996       11996       3  Not what i expected. yet a very interesting bo...   \n",
       "11997       11997       5  Dragon Knights is a world where Knights ride d...   \n",
       "11998       11998       4  Since this story is very short, it's hard to s...   \n",
       "11999       11999       4  from 1922 an amazing collection of info on sym...   \n",
       "\n",
       "                                                 summary  \n",
       "0                                      50 + years ago...  \n",
       "1                                Boring! Boring! Boring!  \n",
       "2                        Wiggleliscious/new toy ready/!!  \n",
       "3                                        Very good read.  \n",
       "4                                           Great Story!  \n",
       "...                                                  ...  \n",
       "11995                                  Where's the meat?  \n",
       "11996                                        Interesting  \n",
       "11997  Dragon Knights, Wings of Change (I Dream of Dr...  \n",
       "11998                          Good writing, short story  \n",
       "11999                     interesting public domain book  \n",
       "\n",
       "[12000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abde367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This book was the very first bookmobile book I bought when I was in the school book club. I loved the story then and I bet a dollar to a donut I will love it again. If my memory serves, I bought this book in 5th grade. That would have been about 1961. I am looking forward to reliving the memories.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54813842",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42083bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "rating        0\n",
       "reviewText    0\n",
       "summary       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edd3064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3000\n",
       "5    3000\n",
       "1    2000\n",
       "2    2000\n",
       "3    2000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelcounts = df['rating'].value_counts()\n",
    "labelcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3939a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline and raw agreement:\n",
      "0.25\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print('baseline and raw agreement:')\n",
    "print(3000/sum(labelcounts))\n",
    "print(1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['reviewText'].tolist()\n",
    "rating = df['rating'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479de1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## comment out to use full data set\n",
    "text=text[:100]\n",
    "rating= rating[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a139ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee93558",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9b92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "textdocuments=[]\n",
    "for item in text:\n",
    "    x = word_tokenize(item)\n",
    "    textdocuments.append(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbee234",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495d78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to take a list \n",
    "#make each token lowercase\n",
    "#place the lowercase tokens into a list\n",
    "def lc(list):\n",
    "    lclist =[]\n",
    "    for token in list:\n",
    "        lcw = token.lower()\n",
    "        lclist.append(lcw) \n",
    "    return lclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fcee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the lowercase function\n",
    "\n",
    "kindle_lc=[]\n",
    "for item in textdocuments:\n",
    "    n = lc(item)\n",
    "    kindle_lc.append(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ac34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to remove punctuation from a list\n",
    "from string import punctuation\n",
    "punctuation = punctuation + '’”“`''`—'\n",
    "def remove_punc(tokens):\n",
    "    return [t for t in tokens if t not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff4e4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_lcp=[]\n",
    "for item in kindle_lc:\n",
    "    remove_punc(item)\n",
    "    kindle_lcp.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09ef4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords=nltkstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22238ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textdocuments=[]\n",
    "#for token in kindle_lcp:\n",
    "#    if token not in stopwords:\n",
    "#        textdocuments.append(token)\n",
    "#    else:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f1e32",
   "metadata": {},
   "source": [
    "##  NLTK BOW classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fed591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textdocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7c30b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(list1, list2):\n",
    "    merged_list = [(list1 [i], list2 [i]) for i in range (0, len(list1))]\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a14fa88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = merge(textdocuments,rating) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff44793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(['This', 'book', 'was', 'the', 'very', 'first', 'bookmobile', 'book', 'I', 'bought', 'when', 'I', 'was', 'in', 'the', 'school', 'book', 'club', '.', 'I', 'loved', 'the', 'story', 'then', 'and', 'I', 'bet', 'a', 'dollar', 'to', 'a', 'donut', 'I', 'will', 'love', 'it', 'again', '.', 'If', 'my', 'memory', 'serves', ',', 'I', 'bought', 'this', 'book', 'in', '5th', 'grade', '.', 'That', 'would', 'have', 'been', 'about', '1961', '.', 'I', 'am', 'looking', 'forward', 'to', 'reliving', 'the', 'memories', '.'], 5)\n"
     ]
    }
   ],
   "source": [
    "print(len(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b708ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "(['This', 'book', 'was', 'the', 'very', 'first', 'bookmobile', 'book', 'I', 'bought', 'when', 'I', 'was', 'in', 'the', 'school', 'book', 'club', '.', 'I', 'loved', 'the', 'story', 'then', 'and', 'I', 'bet', 'a', 'dollar', 'to', 'a', 'donut', 'I', 'will', 'love', 'it', 'again', '.', 'If', 'my', 'memory', 'serves', ',', 'I', 'bought', 'this', 'book', 'in', '5th', 'grade', '.', 'That', 'would', 'have', 'been', 'about', '1961', '.', 'I', 'am', 'looking', 'forward', 'to', 'reliving', 'the', 'memories', '.'], 5)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(type(documents))\n",
    "print(type(documents[0]))\n",
    "print(documents[0])\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38aa737",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50dae79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c957e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords=nltkstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0979a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c935f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'addresses',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'most',\n",
       " 'churches',\n",
       " 'ignore',\n",
       " '.',\n",
       " 'Many',\n",
       " 'people',\n",
       " 'leave',\n",
       " 'organized',\n",
       " 'religion',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'inflicted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'religious',\n",
       " '.',\n",
       " 'Chris',\n",
       " 'Jackson',\n",
       " 'addresses',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'from',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'and',\n",
       " 'offers',\n",
       " 'understanding',\n",
       " 'and',\n",
       " 'healing',\n",
       " 'for',\n",
       " 'the',\n",
       " 'wounded',\n",
       " '.',\n",
       " 'Highly',\n",
       " 'recommend',\n",
       " 'this',\n",
       " 'book',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'has',\n",
       " 'been',\n",
       " 'offended',\n",
       " 'in',\n",
       " 'church',\n",
       " '.',\n",
       " 'A',\n",
       " 'snow',\n",
       " 'storm',\n",
       " 'brought',\n",
       " 'them',\n",
       " 'together',\n",
       " ',',\n",
       " 'but',\n",
       " 'God',\n",
       " \"'s\",\n",
       " 'love',\n",
       " 'and',\n",
       " 'their',\n",
       " 'love',\n",
       " 'for',\n",
       " 'one',\n",
       " 'another',\n",
       " 'will',\n",
       " 'keep',\n",
       " 'them',\n",
       " 'together',\n",
       " 'when',\n",
       " 'deep',\n",
       " 'hurts',\n",
       " ',',\n",
       " 'fears',\n",
       " ',',\n",
       " 'on',\n",
       " 'and',\n",
       " 'sins',\n",
       " 'from',\n",
       " 'the',\n",
       " 'past',\n",
       " 'threaten',\n",
       " 'to',\n",
       " 'tear',\n",
       " 'them',\n",
       " 'apart',\n",
       " '.',\n",
       " 'I',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'this',\n",
       " 'story',\n",
       " 'of',\n",
       " 'forgiveness',\n",
       " 'and',\n",
       " 'new',\n",
       " 'beginnings',\n",
       " '.',\n",
       " 'Ok',\n",
       " 'for',\n",
       " 'a',\n",
       " 'quick',\n",
       " 'read',\n",
       " ',',\n",
       " 'but',\n",
       " 'only',\n",
       " 'the',\n",
       " 'main',\n",
       " 'people',\n",
       " 'stood',\n",
       " 'out',\n",
       " ',',\n",
       " 'so',\n",
       " 'why',\n",
       " 'add',\n",
       " 'other',\n",
       " 'characters',\n",
       " '?',\n",
       " 'Oh',\n",
       " 'well',\n",
       " 'it',\n",
       " 'was',\n",
       " 'ok',\n",
       " 'but',\n",
       " 'still',\n",
       " 'rushed',\n",
       " 'in',\n",
       " 'my',\n",
       " 'opinion.not',\n",
       " 'quite',\n",
       " 'sure',\n",
       " 'what',\n",
       " 'went',\n",
       " 'wrong.but',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'something',\n",
       " 'did',\n",
       " '.',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'the',\n",
       " 'work',\n",
       " 'was',\n",
       " 'boring',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'repetitive',\n",
       " '.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'had',\n",
       " 'no',\n",
       " 'connection',\n",
       " 'to',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'and',\n",
       " 'got',\n",
       " 'bored',\n",
       " 'half',\n",
       " 'way',\n",
       " 'through',\n",
       " '.',\n",
       " 'What',\n",
       " 'you',\n",
       " 'see',\n",
       " 'is',\n",
       " 'what',\n",
       " 'you',\n",
       " 'get',\n",
       " '.',\n",
       " 'A',\n",
       " 'XXX',\n",
       " 'hot',\n",
       " 'Laura',\n",
       " 'Leigh',\n",
       " 'book',\n",
       " 'that',\n",
       " 'will',\n",
       " 'make',\n",
       " 'you',\n",
       " 'pant',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'are',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'is',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'rah',\n",
       " 'rah',\n",
       " 'for',\n",
       " 'women',\n",
       " \"'s\",\n",
       " 'lib',\n",
       " 'or',\n",
       " 'any',\n",
       " 'other',\n",
       " 'political',\n",
       " 'vent',\n",
       " 'bent',\n",
       " 'issue',\n",
       " 'yelling',\n",
       " 'syndrome',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'book',\n",
       " 'for',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'get',\n",
       " 'fired',\n",
       " 'up',\n",
       " 'by',\n",
       " 'yourself',\n",
       " 'or',\n",
       " 'with',\n",
       " 'your',\n",
       " 'significant',\n",
       " 'other',\n",
       " 'for',\n",
       " 'a',\n",
       " 'hot',\n",
       " 'between',\n",
       " 'the',\n",
       " 'sheets',\n",
       " 'session',\n",
       " ',',\n",
       " 'this',\n",
       " 'will',\n",
       " 'definitely',\n",
       " 'help',\n",
       " 'your',\n",
       " 'agenda',\n",
       " '.',\n",
       " 'Just',\n",
       " 'remember',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'Jane',\n",
       " 'Austin',\n",
       " 'material',\n",
       " 'and',\n",
       " 'strictly',\n",
       " 'for',\n",
       " 'those',\n",
       " 'over',\n",
       " '18',\n",
       " 'or',\n",
       " '21',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'state',\n",
       " 'or',\n",
       " 'country',\n",
       " 'you',\n",
       " 'live',\n",
       " 'in',\n",
       " '.',\n",
       " 'I',\n",
       " 'enjoyed',\n",
       " 'this',\n",
       " 'book',\n",
       " 'up',\n",
       " 'until',\n",
       " 'the',\n",
       " 'ending',\n",
       " '.',\n",
       " 'I',\n",
       " 'think',\n",
       " 'the',\n",
       " 'author',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'little',\n",
       " 'more',\n",
       " 'descriptive',\n",
       " 'when',\n",
       " 'she',\n",
       " 'brought',\n",
       " 'this',\n",
       " 'book',\n",
       " 'to',\n",
       " 'an',\n",
       " 'end',\n",
       " '.',\n",
       " 'It',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'amaze',\n",
       " 'me',\n",
       " 'how',\n",
       " 'Verne',\n",
       " 'could',\n",
       " 'write',\n",
       " 'stories',\n",
       " 'and',\n",
       " 'novels',\n",
       " 'with',\n",
       " 'such',\n",
       " 'brilliant',\n",
       " 'forethought',\n",
       " 'and',\n",
       " 'logic',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " '.',\n",
       " 'Did',\n",
       " 'not',\n",
       " 'cut',\n",
       " 'it',\n",
       " '.',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'the',\n",
       " 'way',\n",
       " 'Mr.',\n",
       " 'Hewson',\n",
       " 'writesNot',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'peak',\n",
       " 'my',\n",
       " 'interest',\n",
       " '.',\n",
       " 'Sorry',\n",
       " '.',\n",
       " 'Review',\n",
       " 'contains',\n",
       " 'spoilersThe',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'the',\n",
       " 'story',\n",
       " 'sounded',\n",
       " 'great',\n",
       " '.',\n",
       " 'Two',\n",
       " 'men',\n",
       " 'who',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'together',\n",
       " ',',\n",
       " 'one',\n",
       " 'gay',\n",
       " ',',\n",
       " 'the',\n",
       " 'other',\n",
       " 'straight',\n",
       " 'have',\n",
       " 'to',\n",
       " 'marry/bond',\n",
       " 'for',\n",
       " 'their',\n",
       " 'clans',\n",
       " 'to',\n",
       " 'strike',\n",
       " 'a',\n",
       " 'peace',\n",
       " '.',\n",
       " 'You',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'just',\n",
       " 'guys',\n",
       " 'they',\n",
       " 'are',\n",
       " 'werewolves',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'we',\n",
       " 'have',\n",
       " 'two',\n",
       " 'great',\n",
       " 'cliches',\n",
       " 'in',\n",
       " 'one',\n",
       " 'story',\n",
       " ':',\n",
       " '1',\n",
       " '.',\n",
       " ')',\n",
       " 'forced',\n",
       " 'arranged',\n",
       " 'marriage',\n",
       " 'and',\n",
       " '2',\n",
       " '.',\n",
       " ')',\n",
       " 'paranormal/werewolves.However',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'story',\n",
       " 'has',\n",
       " 'some',\n",
       " 'hot',\n",
       " 'chemistry',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'leads',\n",
       " ',',\n",
       " 'Hayden',\n",
       " 'and',\n",
       " 'Josh',\n",
       " ',',\n",
       " 'the',\n",
       " 'overall',\n",
       " 'drama',\n",
       " 'is',\n",
       " 'told',\n",
       " 'in',\n",
       " 'flashbacks',\n",
       " ',',\n",
       " 'which',\n",
       " 'take',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'present',\n",
       " 'day',\n",
       " 'tale',\n",
       " '.',\n",
       " 'Plus',\n",
       " ',',\n",
       " 'Samhain',\n",
       " 'nor',\n",
       " 'the',\n",
       " 'author',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'warn',\n",
       " 'for',\n",
       " 'the',\n",
       " 'out-of-the-blue',\n",
       " 'rape',\n",
       " 'that',\n",
       " 'occurs',\n",
       " 'to',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heroes',\n",
       " 'by',\n",
       " 'some',\n",
       " 'off-screen',\n",
       " 'criminals',\n",
       " '(',\n",
       " 'which',\n",
       " 'did',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'advance',\n",
       " 'the',\n",
       " 'plot',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'seemed',\n",
       " 'pointless',\n",
       " '.',\n",
       " 'So',\n",
       " 'I',\n",
       " 'can',\n",
       " 'only',\n",
       " 'give',\n",
       " 'the',\n",
       " 'book',\n",
       " '3',\n",
       " 'stars',\n",
       " '.',\n",
       " 'The',\n",
       " 'premise',\n",
       " 'had',\n",
       " 'promise',\n",
       " ',',\n",
       " 'but',\n",
       " 'did',\n",
       " 'not',\n",
       " 'come',\n",
       " 'to',\n",
       " 'full',\n",
       " 'frution',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'material',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Woody',\n",
       " 'Allen',\n",
       " 'movie',\n",
       " '(',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'like',\n",
       " 'his',\n",
       " 'movies',\n",
       " ')',\n",
       " '.',\n",
       " 'David',\n",
       " 'missed',\n",
       " 'Woodstock',\n",
       " ',',\n",
       " 'and',\n",
       " 'messed',\n",
       " 'everything',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'girl',\n",
       " 'he',\n",
       " 'had',\n",
       " 'a',\n",
       " 'super',\n",
       " 'crush',\n",
       " 'on',\n",
       " ',',\n",
       " 'all',\n",
       " 'because',\n",
       " 'he',\n",
       " 'was',\n",
       " 'too',\n",
       " 'cautious',\n",
       " 'and',\n",
       " 'scared',\n",
       " '.',\n",
       " 'He',\n",
       " 'grows',\n",
       " 'up',\n",
       " 'to',\n",
       " 'a',\n",
       " 'boring',\n",
       " 'middle',\n",
       " 'management',\n",
       " 'type',\n",
       " 'job',\n",
       " ',',\n",
       " 'wife',\n",
       " ',',\n",
       " 'kids',\n",
       " 'etc',\n",
       " 'but',\n",
       " 'has',\n",
       " 'never',\n",
       " 'gotton',\n",
       " 'over',\n",
       " 'missing',\n",
       " 'Woodstock.Now',\n",
       " ',',\n",
       " '40',\n",
       " 'years',\n",
       " 'later',\n",
       " 'the',\n",
       " 'crush',\n",
       " 'girl',\n",
       " ',',\n",
       " 'who',\n",
       " 'has',\n",
       " 'become',\n",
       " 'famous',\n",
       " ',',\n",
       " 'has',\n",
       " 'waltzed',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'emailed',\n",
       " 'her',\n",
       " 'way',\n",
       " 'into',\n",
       " 'his',\n",
       " 'life',\n",
       " '.',\n",
       " 'He',\n",
       " 'agonizes',\n",
       " 'over',\n",
       " 'every',\n",
       " 'email',\n",
       " 'he',\n",
       " 'sends',\n",
       " 'her',\n",
       " ',',\n",
       " 'wanting',\n",
       " 'the',\n",
       " 'wording',\n",
       " 'to',\n",
       " 'be',\n",
       " 'perfect',\n",
       " '.',\n",
       " 'He',\n",
       " 'plans',\n",
       " 'to',\n",
       " 'see',\n",
       " 'her',\n",
       " ',',\n",
       " 'and',\n",
       " 'they',\n",
       " 'will',\n",
       " 'attend',\n",
       " 'the',\n",
       " 'Woodstock',\n",
       " 'Reunion',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'will',\n",
       " 'be',\n",
       " 'right',\n",
       " '.',\n",
       " 'Except',\n",
       " 'that',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'what',\n",
       " 'happens',\n",
       " '.',\n",
       " 'Pretty',\n",
       " 'much',\n",
       " 'nothing',\n",
       " 'happens.There',\n",
       " 'are',\n",
       " 'other',\n",
       " 'free',\n",
       " 'books',\n",
       " 'out',\n",
       " 'there',\n",
       " 'that',\n",
       " 'are',\n",
       " 'worthy',\n",
       " 'of',\n",
       " 'more',\n",
       " 'stars',\n",
       " '.',\n",
       " 'I',\n",
       " 'suggest',\n",
       " 'you',\n",
       " 'find',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " '.',\n",
       " 'This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'about',\n",
       " 'a',\n",
       " 'community',\n",
       " 'of',\n",
       " 'witches',\n",
       " '(',\n",
       " 'or',\n",
       " 'persons',\n",
       " 'with',\n",
       " 'magic',\n",
       " 'powers',\n",
       " ')',\n",
       " 'who',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'lose',\n",
       " 'select',\n",
       " 'members',\n",
       " 'to',\n",
       " 'a',\n",
       " 'murderer',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'decent',\n",
       " 'plot',\n",
       " ',',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'are',\n",
       " 'likeable',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'steamy',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'hot',\n",
       " 'enough',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'way',\n",
       " 'to',\n",
       " 'simplistic',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " '.',\n",
       " 'It',\n",
       " 'turns',\n",
       " 'into',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'Scooby-Doo',\n",
       " 'episode',\n",
       " 'were',\n",
       " 'the',\n",
       " 'murderer',\n",
       " 'steps',\n",
       " 'forward',\n",
       " 'and',\n",
       " 'explains',\n",
       " 'all',\n",
       " 'there',\n",
       " 'reasons',\n",
       " 'and',\n",
       " 'then',\n",
       " 'is',\n",
       " 'just',\n",
       " 'taken',\n",
       " 'away',\n",
       " '....',\n",
       " 'umm',\n",
       " 'yawn',\n",
       " 'boring',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'some',\n",
       " 'confusing',\n",
       " 'paragraphs',\n",
       " 'off',\n",
       " 'and',\n",
       " 'on',\n",
       " 'where',\n",
       " 'you',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'is',\n",
       " 'talking',\n",
       " 'and',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " 'referring',\n",
       " 'too',\n",
       " 'otherwise',\n",
       " 'the',\n",
       " 'author',\n",
       " 'can',\n",
       " 'put',\n",
       " 'together',\n",
       " 'a',\n",
       " 'decent',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'mediocre',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'try',\n",
       " 'another',\n",
       " 'book',\n",
       " 'from',\n",
       " 'this',\n",
       " 'author',\n",
       " 'but',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'expect',\n",
       " 'a',\n",
       " 'lot',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'read',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'hours',\n",
       " 'at',\n",
       " 'most',\n",
       " '.',\n",
       " 'When',\n",
       " 'I',\n",
       " 'say',\n",
       " 'this',\n",
       " 'was',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'read',\n",
       " ',',\n",
       " 'I',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'I',\n",
       " 'had',\n",
       " 'to',\n",
       " 'push',\n",
       " 'myself',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'book',\n",
       " '.',\n",
       " 'It',\n",
       " 'did',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'as',\n",
       " 'it',\n",
       " 'went',\n",
       " 'on',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'was',\n",
       " 'dragging',\n",
       " '.',\n",
       " 'I',\n",
       " 'figured',\n",
       " 'that',\n",
       " 'as',\n",
       " 'long',\n",
       " 'as',\n",
       " 'I',\n",
       " 'paid',\n",
       " 'for',\n",
       " 'the',\n",
       " 'book',\n",
       " 'I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'reading',\n",
       " 'it',\n",
       " '.',\n",
       " 'If',\n",
       " 'I',\n",
       " 'try',\n",
       " 'to',\n",
       " 'read',\n",
       " 'another',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'books',\n",
       " ',',\n",
       " 'I',\n",
       " 'will',\n",
       " 'get',\n",
       " 'it',\n",
       " 'from',\n",
       " 'the',\n",
       " 'library',\n",
       " 'for',\n",
       " 'free',\n",
       " '.',\n",
       " 'As',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'by',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reviews',\n",
       " 'and',\n",
       " 'the',\n",
       " 'low',\n",
       " 'ranking',\n",
       " 'of',\n",
       " 'this',\n",
       " 'title',\n",
       " ',',\n",
       " 'readers',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'this',\n",
       " 'marketing',\n",
       " 'tactic',\n",
       " 'for',\n",
       " 'ebooks.Samples',\n",
       " 'may',\n",
       " 'work',\n",
       " 'well',\n",
       " 'in',\n",
       " 'the',\n",
       " 'grocery',\n",
       " 'store',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'do',\n",
       " 'not',\n",
       " 'fare',\n",
       " 'the',\n",
       " 'same',\n",
       " 'when',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'ebooks',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'the',\n",
       " 'option',\n",
       " 'to',\n",
       " 'download',\n",
       " 'samples',\n",
       " 'to',\n",
       " 'our',\n",
       " 'ereaders.Potential',\n",
       " 'readers',\n",
       " 'will',\n",
       " 'glance',\n",
       " 'at',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'in',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'options',\n",
       " ',',\n",
       " 'see',\n",
       " 'the',\n",
       " 'two',\n",
       " 'or',\n",
       " 'one',\n",
       " 'star',\n",
       " 'average',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'on',\n",
       " 'going',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'their',\n",
       " 'mind',\n",
       " 'they',\n",
       " 'will',\n",
       " 'think',\n",
       " '-',\n",
       " 'this',\n",
       " 'author',\n",
       " 'must',\n",
       " 'stink',\n",
       " '.',\n",
       " 'Instead',\n",
       " ',',\n",
       " 'I',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c3b3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdocuments=[]\n",
    "for token in all_words_list:\n",
    "    if token not in stopwords:\n",
    "        textdocuments.append(token)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b59e610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addresses'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdocuments[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1925fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'to', 'I', 'a', 'of', 'is', 'it', 'in', 'this', 'was', 'that', \"'s\", 'for', 'book', 'her', 'but', 'with', \"n't\", 'he', '!', 'story', 'read', 'not', 'be', 'on', 'just', 'as', 'his', 'one', 'you', 'have', 'she', ')', '(', 'my', 'The', 'love', 'has', 'more', 'are', 'me', 'all', 'at', 'from', 'who', 'It', 'first']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "#   note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6bd4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c90647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59849b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feature sets are 2000 words long so you may not want to look at one\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2304e",
   "metadata": {},
   "source": [
    "## Print to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5925dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFeatureSets(featuresets, outpath):\n",
    "    # open outpath for writing\n",
    "    f = open(outpath, 'w')\n",
    "    # get the feature names from the feature dictionary in the first featureset\n",
    "    featurenames = featuresets[0][0].keys()\n",
    "    # create the first line of the file as comma separated feature names\n",
    "    #    with the word class as the last feature name\n",
    "    featurenameline = ''\n",
    "    for featurename in featurenames:\n",
    "        # replace forbidden characters with text abbreviations\n",
    "        featurename = featurename.replace(',','CM')\n",
    "        featurename = featurename.replace(\"'\",\"DQ\")\n",
    "        featurename = featurename.replace('\"','QU')\n",
    "        featurenameline += featurename + ','\n",
    "    featurenameline += 'class'\n",
    "    # write this as the first line in the csv file\n",
    "    f.write(featurenameline)\n",
    "    f.write('\\n')\n",
    "    # convert each feature set to a line in the file with comma separated feature values,\n",
    "    # each feature value is converted to a string \n",
    "    #   for booleans this is the words true and false\n",
    "    #   for numbers, this is the string with the number\n",
    "    for featureset in featuresets:\n",
    "        featureline = ''\n",
    "        for key in featurenames:\n",
    "            featureline += str(featureset[0][key]) + ','\n",
    "        featureline += str(featureset[1])\n",
    "        # write each feature set values to the file\n",
    "        f.write(featureline)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d240afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFeatureSets(featuresets,r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\featuresets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b98304",
   "metadata": {},
   "source": [
    "## Inport CSV to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec31338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset_df = pd.read_csv(r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\featuresets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4206390c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V_the</th>\n",
       "      <th>V_.</th>\n",
       "      <th>V_CM</th>\n",
       "      <th>V_and</th>\n",
       "      <th>V_to</th>\n",
       "      <th>V_I</th>\n",
       "      <th>V_a</th>\n",
       "      <th>V_of</th>\n",
       "      <th>V_is</th>\n",
       "      <th>V_it</th>\n",
       "      <th>...</th>\n",
       "      <th>V_subscription</th>\n",
       "      <th>V_12/mo</th>\n",
       "      <th>V_single</th>\n",
       "      <th>V_.99</th>\n",
       "      <th>V_Gave</th>\n",
       "      <th>V_papers</th>\n",
       "      <th>V_subscribing</th>\n",
       "      <th>V_Sounded</th>\n",
       "      <th>V_promising</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V_the   V_.   V_CM  V_and   V_to    V_I    V_a   V_of   V_is   V_it  ...  \\\n",
       "0    True  True  False   True  False  False   True   True  False  False  ...   \n",
       "1    True  True   True   True   True   True  False   True  False  False  ...   \n",
       "2    True  True   True  False  False   True   True  False  False   True  ...   \n",
       "3    True  True  False   True   True   True   True  False  False  False  ...   \n",
       "4    True  True   True   True   True  False   True  False   True  False  ...   \n",
       "..    ...   ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "95   True  True   True   True   True   True   True   True   True  False  ...   \n",
       "96   True  True   True   True   True   True   True  False  False   True  ...   \n",
       "97  False  True  False  False  False   True   True   True  False  False  ...   \n",
       "98   True  True  False  False   True   True  False  False   True   True  ...   \n",
       "99   True  True   True   True   True   True   True   True  False   True  ...   \n",
       "\n",
       "    V_subscription  V_12/mo  V_single  V_.99  V_Gave  V_papers  V_subscribing  \\\n",
       "0            False    False     False  False   False     False          False   \n",
       "1            False    False     False  False   False     False          False   \n",
       "2            False    False     False  False   False     False          False   \n",
       "3            False    False     False  False   False     False          False   \n",
       "4            False    False     False  False   False     False          False   \n",
       "..             ...      ...       ...    ...     ...       ...            ...   \n",
       "95           False    False     False  False   False     False          False   \n",
       "96           False    False     False  False   False     False          False   \n",
       "97           False    False     False  False   False     False          False   \n",
       "98           False    False     False  False   False     False          False   \n",
       "99           False    False     False  False   False     False          False   \n",
       "\n",
       "    V_Sounded  V_promising  class  \n",
       "0       False        False      5  \n",
       "1       False        False      5  \n",
       "2       False        False      3  \n",
       "3       False        False      1  \n",
       "4       False        False      3  \n",
       "..        ...          ...    ...  \n",
       "95      False        False      4  \n",
       "96      False        False      5  \n",
       "97      False        False      2  \n",
       "98      False        False      2  \n",
       "99      False        False      1  \n",
       "\n",
       "[100 rows x 2001 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c9998",
   "metadata": {},
   "source": [
    "## Creating MNB model on featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a821d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs = featureset_df.drop('class', axis=1)\n",
    "\n",
    "y_fs = featureset_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34a79bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "# initialize the model\n",
    "mnb1_fs = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48acd9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    }
   ],
   "source": [
    "# use the training data to train the model\n",
    "mnb_model_fs = mnb1_fs.fit(X_fs,y_fs)\n",
    "#cross validation score\n",
    "mnb_cv_scores_fs = cross_val_score(mnb1_fs, X_fs, y_fs, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "mnb_cv_score_fs = np.mean(mnb_cv_scores_fs)\n",
    "print(mnb_cv_score_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e6e9d",
   "metadata": {},
   "source": [
    "##  Adding Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "439b38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for using bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e73f68e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'addresses', 'a', 'problem', 'most', 'churches', 'ignore', '.', 'Many', 'people', 'leave', 'organized', 'religion', 'because', 'of', 'the', 'pain', 'inflicted', 'by', 'the', 'religious', '.', 'Chris', 'Jackson', 'addresses', 'the', 'problem', 'from', 'both', 'sides', 'and', 'offers', 'understanding', 'and', 'healing', 'for', 'the', 'wounded', '.', 'Highly', 'recommend', 'this', 'book', 'for', 'anyone', 'who', 'has', 'been', 'offended']\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder on all the words in sequence\n",
    "print(all_words_list[:50])\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea8e1f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'D\", 'DO'), (\"'M\", 'WELL'), (\"'Plan\", 'Nine'), (\"'S\", 'meat'), ('.............................', 'ends.Why'), ('/', 'December'), ('/eyeroll/', 'Put'), ('0.99', 'fiction'), ('3-day', 'getaway'), ('35', 'minutes.Bleh.Being'), ('5th', 'grade'), ('ABSOLUTE', 'KEEPER'), ('AN', 'ABSOLUTE'), ('Allen', 'movie'), ('Aunt', 'Sophie'), ('BACK', 'HIS'), ('BECHETT', 'DeSAXBY'), ('BROUGHT', 'BACK'), ('Bewitching', 'Mysteries'), ('Blue', 'Fish'), ('Box', 'Set'), ('Brenda', 'seems'), ('CASTLE', \"'S\"), ('Civil', 'War'), ('Confederate', 'Colonel'), ('Cousin', 'Vinnie'), ('Creek', 'ensemble'), ('Crouch', 'wrote'), ('DANCE', 'IN'), ('DEFINATELY', 'purchasing'), ('Dark', 'Side'), ('Darkest', 'Night'), ('David', 'missed'), ('Debra', 'Glass'), ('Diana', 'Gabaldon'), ('Elle', 'Kennedy.If'), ('Emma', 'Darcy'), ('FILET', 'MIGNON'), ('FOR', 'ME'), ('Fish', 'Grill'), ('Francis', 'Ray'), ('HE', 'WENT'), ('HEART', 'RATE'), ('HIS', 'CASTLE'), ('Halloween', 'season'), ('Hat', 'On'), ('Hero/daydream', 'inducing'), ('Hewson', 'writesNot'), ('Identical', 'twin'), ('JUST', 'PICKED')]\n"
     ]
    }
   ],
   "source": [
    "# define the top 500 bigrams using the chi squared measure\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "print(bigram_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "267f7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include words as before \n",
    "# add the most frequent significant bigrams\n",
    "# this function takes the list of words in a document as an argument and returns a feature dictionary\n",
    "# it depends on the variables word_features and bigram_features\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69a95298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd6fda",
   "metadata": {},
   "source": [
    "## Wrting bigram features sets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "392b7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writeFeatureSets(bigram_featuresets,r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\bigram_featuresets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69898948",
   "metadata": {},
   "source": [
    "## Importing bigram features sets to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdd5fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_featuresets_df = pd.read_csv(r'C:\\Users\\Morga\\programsMG\\NaturalLanguageProcessing\\bigram_featuresets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4c1c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V_.</th>\n",
       "      <th>V_the</th>\n",
       "      <th>V_CM</th>\n",
       "      <th>V_and</th>\n",
       "      <th>V_to</th>\n",
       "      <th>V_a</th>\n",
       "      <th>V_I</th>\n",
       "      <th>V_of</th>\n",
       "      <th>V_is</th>\n",
       "      <th>V_it</th>\n",
       "      <th>...</th>\n",
       "      <th>B_Talkative_TortoiseA</th>\n",
       "      <th>B_Tars_Tarkas</th>\n",
       "      <th>B_Tatty_MouseJack</th>\n",
       "      <th>B_Test_Pilot</th>\n",
       "      <th>B_Thai_Coconut</th>\n",
       "      <th>B_Thi_swas</th>\n",
       "      <th>B_Thrift_Editions</th>\n",
       "      <th>B_Tic_Tac</th>\n",
       "      <th>B_Tick_Tock</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V_.  V_the   V_CM  V_and   V_to   V_a    V_I   V_of   V_is   V_it  \\\n",
       "0      True   True   True   True   True  True   True  False  False   True   \n",
       "1      True   True   True   True   True  True   True   True   True   True   \n",
       "2      True   True   True   True   True  True   True   True   True   True   \n",
       "3      True   True   True   True   True  True   True  False  False   True   \n",
       "4      True   True   True   True   True  True   True   True   True  False   \n",
       "...     ...    ...    ...    ...    ...   ...    ...    ...    ...    ...   \n",
       "11995  True  False  False  False   True  True   True  False  False  False   \n",
       "11996  True  False   True  False   True  True  False   True  False  False   \n",
       "11997  True   True  False   True  False  True   True   True   True   True   \n",
       "11998  True   True   True   True   True  True   True   True   True   True   \n",
       "11999  True   True   True   True  False  True  False   True  False  False   \n",
       "\n",
       "       ...  B_Talkative_TortoiseA  B_Tars_Tarkas  B_Tatty_MouseJack  \\\n",
       "0      ...                  False          False              False   \n",
       "1      ...                  False          False              False   \n",
       "2      ...                  False          False              False   \n",
       "3      ...                  False          False              False   \n",
       "4      ...                  False          False              False   \n",
       "...    ...                    ...            ...                ...   \n",
       "11995  ...                  False          False              False   \n",
       "11996  ...                  False          False              False   \n",
       "11997  ...                  False          False              False   \n",
       "11998  ...                  False          False              False   \n",
       "11999  ...                  False          False              False   \n",
       "\n",
       "       B_Test_Pilot  B_Thai_Coconut  B_Thi_swas  B_Thrift_Editions  B_Tic_Tac  \\\n",
       "0             False           False       False              False      False   \n",
       "1             False           False       False              False      False   \n",
       "2             False           False       False              False      False   \n",
       "3             False           False       False              False      False   \n",
       "4             False           False       False              False      False   \n",
       "...             ...             ...         ...                ...        ...   \n",
       "11995         False           False       False              False      False   \n",
       "11996         False           False       False              False      False   \n",
       "11997         False           False       False              False      False   \n",
       "11998         False           False       False              False      False   \n",
       "11999         False           False       False              False      False   \n",
       "\n",
       "       B_Tick_Tock  class  \n",
       "0            False      5  \n",
       "1            False      1  \n",
       "2            False      5  \n",
       "3            False      5  \n",
       "4            False      5  \n",
       "...            ...    ...  \n",
       "11995        False      2  \n",
       "11996        False      3  \n",
       "11997        False      5  \n",
       "11998        False      4  \n",
       "11999        False      4  \n",
       "\n",
       "[12000 rows x 2501 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_featuresets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "558f4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bifs = bigram_featuresets_df.drop('class', axis=1)\n",
    "\n",
    "y_bifs = bigram_featuresets_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e25475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "# initialize the model\n",
    "mnb1_bifs = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd8bdc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49458333333333326\n"
     ]
    }
   ],
   "source": [
    "# use the training data to train the model\n",
    "mnb_model_bifs = mnb1_bifs.fit(X_bifs,y_bifs)\n",
    "#cross validation score\n",
    "mnb_cv_scores_bifs = cross_val_score(mnb1_bifs, X_bifs, y_bifs, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "mnb_cv_score_bifs = np.mean(mnb_cv_scores_bifs)\n",
    "print(mnb_cv_score_bifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3b363",
   "metadata": {},
   "source": [
    "## sklearn approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0248987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the label\n",
    "y=df['rating'].values\n",
    "X=df['reviewText'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b53e71",
   "metadata": {},
   "source": [
    "## MNB unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d228c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the  module\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "# initialize the  model\n",
    "\n",
    "mnb1 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae9cd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "unigram_tfidf_vectorizer_mnb = TfidfVectorizer(encoding='latin-1'\n",
    "                                                 , use_idf=True\n",
    "                                                 , min_df=21 \n",
    "                                                 ,stop_words='english'\n",
    "                                                 , max_features=2450\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a78a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and transforming the training data using the count vectorizer\n",
    "X_train_mnb = unigram_tfidf_vectorizer_mnb.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5527a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49250000000000005\n"
     ]
    }
   ],
   "source": [
    "# use the training data to train the model\n",
    "mnb_model = mnb1.fit(X_train_mnb,y)\n",
    "#cross validation score\n",
    "mnb_cv_scores = cross_val_score(mnb1, X_train_mnb, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "mnb_cv_score = np.mean(mnb_cv_scores)\n",
    "print(mnb_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2a768",
   "metadata": {},
   "source": [
    "## SVM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b39d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the  model\n",
    "svm1 = LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eed9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "unigram_tfidf_vectorizer_svm = TfidfVectorizer(encoding='latin-1'\n",
    "                                            , use_idf=True\n",
    "                                            , min_df=1\n",
    "                                            , stop_words='english'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ad24cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and transforming the training data using the count vectorizer\n",
    "X_train_svm = unigram_tfidf_vectorizer_svm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "672e46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475\n"
     ]
    }
   ],
   "source": [
    "# use the training data to train the model\n",
    "svm_model = svm1.fit(X_train_svm,y)\n",
    "#cross validation score\n",
    "svm_cv_scores = cross_val_score(svm1, X_train_svm, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "svm_cv_score = np.mean(svm_cv_scores)\n",
    "print(svm_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cf8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35ef45ab",
   "metadata": {},
   "source": [
    "## MNB Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "349f0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the  model\n",
    "mnb2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "440d6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ngram_tfidf_vectorizer_mnb = TfidfVectorizer(encoding='latin-1'\n",
    "                                                 ,ngram_range=(1,2)\n",
    "                                                 , use_idf=True\n",
    "                                                 , min_df=5\n",
    "                                                 , max_features=3000\n",
    "                                                 ,stop_words='english'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc33a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and transforming the training data using the count vectorizer\n",
    "X_train_mnb_2 = ngram_tfidf_vectorizer_mnb.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training data to train the model\n",
    "mnb_model_2 = mnb2.fit(X_train_mnb_2,y)\n",
    "#cross validation score\n",
    "mnb_cv_scores_2 = cross_val_score(mnb2, X_train_mnb_2, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "mnb_cv_score_2 = np.mean(mnb_cv_scores_2)\n",
    "print(mnb_cv_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a729031",
   "metadata": {},
   "source": [
    "## SVM Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fe220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the  model\n",
    "svm2 = LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6281be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ngram_tfidf_vectorizer_svm = TfidfVectorizer(encoding='latin-1'\n",
    "                                            , ngram_range=(1,2)\n",
    "                                            , use_idf=True\n",
    "                                            , max_df=.5\n",
    "                                            , stop_words='english'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and transforming the training data using the count vectorizer\n",
    "X_train_svm_2 = ngram_tfidf_vectorizer_svm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b26c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training data to train the model\n",
    "svm_model_2 = svm2.fit(X_train_svm_2,y)\n",
    "#cross validation score\n",
    "svm_cv_scores_2 = cross_val_score(svm2, X_train_svm_2, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "svm_cv_score_2 = np.mean(svm_cv_scores_2)\n",
    "print(svm_cv_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621f9fc",
   "metadata": {},
   "source": [
    "## Accuracy Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acclist={'mnb_cv_score_fs':mnb_cv_score_fs\n",
    "         ,'mnb_cv_score':mnb_cv_score\n",
    "         ,'mnb_cv_score_bifs':mnb_cv_score_bifs\n",
    "         ,'mnb_cv_score_2':mnb_cv_score_2\n",
    "         ,'svm_cv_score':svm_cv_score\n",
    "         ,'svm_cv_score_2':svm_cv_score_2\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb65c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in acclist:\n",
    "    print(item,acclist[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e1c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7a6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP RUN ALL\n",
    "\n",
    "dict={}\n",
    "\n",
    "for item in dict.values()\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab120b7e",
   "metadata": {},
   "source": [
    "## Adding Adjective Phrase counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "kindle_sent_token =[]\n",
    "for item in X:\n",
    "    x = nltk.sent_tokenize(item)\n",
    "    kindle_sent_token.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_sent_token[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d874d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization inside each tokenized sentence\n",
    "kindle_toke = []\n",
    "for item in kindle_sent_token:\n",
    "    for sent in item:\n",
    "        x = nltk.word_tokenize(sent)\n",
    "    kindle_toke.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Stanford POS tagger to POS tag tokens of each sentence\n",
    "# this is the default tagger in nltk\n",
    "tagged_kindle_text = [nltk.pos_tag(tokens) for tokens in kindle_toke]\n",
    "print(tagged_kindle_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc839fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following our NLTK textbook, chapter on Information Extraction--Chunking (https://www.nltk.org/book/ch07.html)\n",
    "\n",
    "# Using CHUNKING to parse sentences \n",
    "# to look for \"adjective phrases\", i.e. phrases (or chunks) that have adverbs and adjectives ('RB'+'JJ')\n",
    "# First step: writing a grammar that defines the POS in the chunk\n",
    "# we name this grammar \"ADJPH\" (\"ADJective PHrase\") using regexes \n",
    "\n",
    "import re\n",
    "grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\"\n",
    "# This regex reads as: \"find groups (\"< >\") of RBs (adverbs) together with groups of JJs (adjectives), with groups defineds as\n",
    "# RBs with any ending (the \".\" is a placeholder or wildcard for the \"R\" and the \"S\" at the end of RBR and RBS, \n",
    "# while \"?\" indicates \"optional character\" so RB can be found alone as well). Same regex operators apply to JJs.\n",
    "\n",
    "# Second step: import the nltk parser to process each sentence\n",
    "chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\n",
    "\n",
    "kindle_adjph_tags = []\n",
    "for sent in tagged_kindle_text:\n",
    "    if len(sent) > 0:\n",
    "        tree = chunk_parser_adj.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'ADJPH':\n",
    "                kindle_adjph_tags.append(subtree)\n",
    "                \n",
    "# Visualizing the actual adjective phrase\n",
    "kindle_adjective_phrases = []\n",
    "for sent in kindle_adjph_tags:\n",
    "    temp = ''\n",
    "    for w, t in sent:\n",
    "        temp += w+ ' '    \n",
    "    kindle_adjective_phrases.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "# Following our NLTK textbook, chapter 1 on Language Processing (https://www.nltk.org/book/ch01.html)\n",
    "\n",
    "## FREQUENCY DISTRIBUTIONS\n",
    "# Top 50 adjective phrases\n",
    "kindle_freq_adjph = nltk.FreqDist(kindle_adjective_phrases)\n",
    "\n",
    "print('Top adjective phrases by frequency: ')\n",
    "adjlist = []\n",
    "for word, freq in kindle_freq_adjph.most_common(50):\n",
    "    adjlist.append(word)\n",
    "\n",
    "            \n",
    "#print the list of our sentences:\n",
    "print('Length of adjective phrase sentences: ', len(kindle_adjph_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28896228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjcheck(list1,substring):\n",
    "    adjcount=[]\n",
    "    for item in list1:\n",
    "        count = item.count(substring)\n",
    "        adjcount.append(count)\n",
    "    return adjcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4815c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Highly_recommended = adjcheck(text,'Highly recommended')\n",
    "really_good = adjcheck(text,'really good')\n",
    "too_much =   adjcheck(text,'too much')\n",
    "really_bad =  adjcheck(text,'really bad')\n",
    "well_developed  =  adjcheck(text,'well developed')\n",
    "very_disappointed  =  adjcheck(text,'very disappointed')\n",
    "Definitely_worth  =  adjcheck(text,'Definitely worth')\n",
    "more_believable  =  adjcheck(text,'more believable')\n",
    "very_entertaining  =  adjcheck(text,'very entertaining')\n",
    "very_good  =   adjcheck(text,'very good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f28e1a",
   "metadata": {},
   "source": [
    "## Adding new feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96243d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "unigram_tfidf_vectorizer_svm_3 = TfidfVectorizer(encoding='latin-1'\n",
    "                                            , use_idf=True\n",
    "                                            , min_df=1\n",
    "                                            , stop_words='english'\n",
    "                                            )\n",
    "#vectorizing using count\n",
    "vecs = unigram_tfidf_vectorizer_svm_3.fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c012c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe for visual inspection\n",
    "pd.set_option('display.max_columns', None)\n",
    "vecsdf=pd.DataFrame(vecs.toarray(),\n",
    "            columns=unigram_tfidf_vectorizer_svm_3.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsdf['Highly_recommended']=Highly_recommended\n",
    "vecsdf['really_good']=really_good\n",
    "vecsdf['too_much']=too_much\n",
    "vecsdf['really_bad']=really_bad\n",
    "vecsdf['well_developed']=well_developed\n",
    "vecsdf['very_disappointed']=very_disappointed\n",
    "vecsdf['Definitely_worth']=Definitely_worth\n",
    "vecsdf['more_believable']=more_believable\n",
    "vecsdf['very_entertaining']=very_entertaining\n",
    "vecsdf['very_good']=very_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c15ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0045537",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3 = LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc942c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training data to train the model\n",
    "svm_model_3 = svm3.fit(vecsdf,y)\n",
    "#cross validation score\n",
    "svm_cv_scores_3 = cross_val_score(svm3, vecsdf, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "svm_cv_score_3 = np.mean(svm_cv_scores_3)\n",
    "print(svm_cv_score_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd0a0a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "unigram_tfidf_vectorizer_mnb_3 = TfidfVectorizer(encoding='latin-1'\n",
    "                                                 , use_idf=True\n",
    "                                                 , min_df=21 \n",
    "                                                 ,stop_words='english'\n",
    "                                                 , max_features=2450\n",
    "                                                )\n",
    "\n",
    "#vectorizing using count\n",
    "vecs2 = unigram_tfidf_vectorizer_mnb_3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaff038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe for visual inspection\n",
    "pd.set_option('display.max_columns', None)\n",
    "vecs2df=pd.DataFrame(vecs2.toarray(),\n",
    "            columns=unigram_tfidf_vectorizer_mnb_3.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsdf['Highly_recommended']=Highly_recommended\n",
    "#vecsdf['really_good']=really_good\n",
    "#vecsdf['too_much']=too_much\n",
    "#vecsdf['really_bad']=really_bad\n",
    "#vecsdf['well_developed']=well_developed\n",
    "#vecsdf['very_disappointed']=very_disappointed\n",
    "#vecsdf['Definitely_worth']=Definitely_worth\n",
    "#vecsdf['more_believable']=more_believable\n",
    "#vecsdf['very_entertaining']=very_entertaining\n",
    "#vecsdf['very_good']=very_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb3 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training data to train the model\n",
    "mnb_model_3 = mnb3.fit(vecs2df,y)\n",
    "#cross validation score\n",
    "mnb_cv_scores_3 = cross_val_score(mnb3, vecs2df, y, cv=10)\n",
    "#finding the overall average accuracy.\n",
    "mnb_cv_score_3 = np.mean(mnb_cv_scores_3)\n",
    "print(mnb_cv_score)\n",
    "print(mnb_cv_score_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681eed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
